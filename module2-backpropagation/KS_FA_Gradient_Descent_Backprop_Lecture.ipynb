{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation & Gradient Descent (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement gradient descent + backpropagation on a feedforward neural network. \n",
    "* <a href=\"#p3\">Part 3</a>: Introduce the Keras Sequential Model API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Calculating *\"cost\"*, *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overall quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train.\n",
    "\n",
    "1. You need more data to train them on. \n",
    "2. They're complex so they take longer to train. \n",
    "3. They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In this section, we will again implement a multi-layer perceptron using numpy. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6",
    "toc-hr-collapsed": true
   },
   "source": [
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# Imagine that our data is drawn from a linear function\n",
    "# y = 2*hours_studying + 4*hours_sleeping + 50\n",
    "\n",
    "# hours studying, hours sleep\n",
    "X = np.array(([2,9],\n",
    "              [1,5],\n",
    "              [3,6]), dtype=float)\n",
    "\n",
    "# Exam Scores\n",
    "y = np.array(([90],\n",
    "              [72],\n",
    "              [80]), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDeUBW6k4Ri4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying, Sleeping \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Test Score \n",
      " [[0.9 ]\n",
      " [0.72]\n",
      " [0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        self.weights1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Randomly Initialize Weights\n",
    "How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IreIDe6P8H0H"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights: \n",
      " [[ 2.48783189  0.11697987 -1.97118428]\n",
      " [-0.48325593 -1.50361209  0.57515126]]\n",
      "Layer 2 weights: \n",
      " [[-0.20672583]\n",
      " [ 0.41271104]\n",
      " [-0.57757999]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
    "print(\"Layer 2 weights: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gGivpEk-VdP"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        # Input to Hidden (1st set of weights)\n",
    "        self.weights1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        # Hidden to Output (2nd set of weights)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activate\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted sum of activated hidden (which output layer will use)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Activation of Output (My Predictions)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 1.        ],\n",
       "       [0.33333333, 0.55555556],\n",
       "       [1.        , 0.66666667]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 1.        ]\n",
      "output [0.25814933]\n"
     ]
    }
   ],
   "source": [
    "# Try to make a prediction with our updated 'net\n",
    "nn = NeuralNetwork()\n",
    "print(X[0])\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64185067])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y[0] - output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]]\n",
      "[[0.64185067]\n",
      " [0.38932808]\n",
      " [0.57357924]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "### Implement Backpropagation \n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1\n",
      " [[-1.75351135  1.23279898  0.24464757]\n",
      " [-0.06568225  0.30190098  0.79723428]] \n",
      "---------\n",
      "hidden_sum\n",
      " [[-1.23468981  1.12376697  0.96033266]\n",
      " [-0.62099392  0.57865576  0.52445712]\n",
      " [-1.79729952  1.4340663   0.77613709]] \n",
      "---------\n",
      "activated_hidden\n",
      " [[0.22536165 0.75468678 0.7231884 ]\n",
      " [0.34955543 0.64075804 0.6281894 ]\n",
      " [0.14218011 0.8075341  0.68484697]] \n",
      "---------\n",
      "weights2\n",
      " [[ 1.23073545]\n",
      " [-1.52187331]\n",
      " [-0.25502715]] \n",
      "---------\n",
      "activated_output\n",
      " [[0.25814933]\n",
      " [0.33067192]\n",
      " [0.22642076]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in attributes if i[:2]!= '__'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX"
   },
   "source": [
    "### Backpropagation (Simple Overview)\n",
    "\n",
    "Our model has 9 total weights (6 in the first layer, 3 in the last layer) that could be off.\n",
    "\n",
    "1) Calculate Error for a given each observation\n",
    "\n",
    "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3) Look at final layer weights to get an idea for which weights are helping pass desirable signals and which are stifling desirable signals\n",
    "\n",
    "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "Repeat steps 2-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Set up Arch\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initialize Weights\n",
    "        # 2x3\n",
    "        # Input to Hidden (1st set of weights)\n",
    "        self.weights1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
    "        \n",
    "        # 3x1\n",
    "        # Hidden to Output (2nd set of weights)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted Sum\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activate\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted sum of activated hidden (which output layer will use)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Activation of Output (My Predictions)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Back prop thru the network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.o_error = y - o # Error in the output\n",
    "        \n",
    "        # Apply derivative of sigmoid to error\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error: how much were our output layer weights off\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # z2 delta: how much were the weights off?\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.output_sum)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "    def train(self, X,y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Let's look at the shape of the Gradient Componets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Error Associated with Each Observation \n",
    "aka how wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60746115],\n",
       "       [0.46613403],\n",
       "       [0.53667427]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Gradient \n",
    "Simple interpretation - how much more sigmoid activation would have pushed us towards the right answer?\n",
    "\n",
    "`self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14866196],\n",
       "       [0.11467591],\n",
       "       [0.13186936]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the derivate of the sigmoid function to understand what's happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "line_x = np.arange(-5, 5, 0.01)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n",
    "\n",
    "# sigmoid\n",
    "y = sigmoid(line_x)\n",
    "y_d = sigmoid_derivative(line_x)\n",
    "\n",
    "x = nn.output_sum\n",
    "s = sigmoid(x)\n",
    "sx = sigmoid_derivative(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# call regplot on each axes\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(15,5))\n",
    "sns.lineplot(x=line_x, y=y, ax=ax1)\n",
    "ax1.plot(x[0], s[0], 'ro')\n",
    "ax1.set_title(\"Sigmoid of Weighted Sum\")\n",
    "sns.lineplot(x=line_x, y=y_d, ax=ax2) \n",
    "ax2.plot(x[0],sx[0],'ro');\n",
    "ax2.set_title(\"Sigmoid Derivative of Weighted Sum\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the derivate graph. The derivative multiplied by the error tells us where to assign blame and update the weights most effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Error\n",
    "Justice hasn't been served yet - tho. We still have neurons to blame. Let's go back another layer. \n",
    "\n",
    "`self.z2_error = self.o_delta.dot(self.weights2.T)`\n",
    "\n",
    "__Discussion:__ Why is this shape different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23952061, -0.19347426,  0.1111231 ],\n",
       "       [-0.1847631 , -0.14924354,  0.08571892],\n",
       "       [-0.21246478, -0.17161974,  0.09857082]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_delta.dot(nn.weights2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Gradient\n",
    "For each observation, how much more sigmoid activation from this layer would have pushed us towards the right answer?\n",
    "\n",
    "`self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05483346, -0.04408088,  0.01744341],\n",
       "       [-0.03871267, -0.0311213 ,  0.01231513],\n",
       "       [-0.04559029, -0.03665025,  0.01450301]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.shape == nn.weights1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descent\n",
    "\n",
    "*Discussion:* Input to Hidden Weight Update\n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.33333333, 1.        ],\n",
       "       [1.        , 0.55555556, 0.66666667]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09505015, -0.07641127,  0.03023699],\n",
       "       [-0.10673402, -0.08580399,  0.03395382]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(nn.z2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* Hidden to Output Weight Update\n",
    "- Why is output the shape 3x1? \n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17103686],\n",
       "       [0.13129211],\n",
       "       [0.18053762]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.activated_hidden.T.dot(nn.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network (fo real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [0.00669285 0.00675966 0.00682713 0.00689527 0.00696409 0.00703359\n",
      " 0.00710377 0.00717466 0.00724624 0.00731853 0.00739154 0.00746527\n",
      " 0.00753973 0.00761493 0.00769088 0.00776757 0.00784502 0.00792324\n",
      " 0.00800223 0.00808201 0.00816257 0.00824393 0.00832609 0.00840907\n",
      " 0.00849286 0.00857749 0.00866294 0.00874925 0.0088364  0.00892442\n",
      " 0.0090133  0.00910306 0.00919371 0.00928525 0.00937769 0.00947104\n",
      " 0.00956532 0.00966052 0.00975667 0.00985376 0.0099518  0.01005081\n",
      " 0.0101508  0.01025177 0.01035374 0.01045671 0.01056069 0.01066569\n",
      " 0.01077173 0.01087881 0.01098694 0.01109614 0.01120641 0.01131776\n",
      " 0.0114302  0.01154375 0.01165842 0.01177421 0.01189113 0.0120092\n",
      " 0.01212843 0.01224883 0.01237041 0.01249319 0.01261716 0.01274235\n",
      " 0.01286876 0.01299642 0.01312532 0.01325548 0.01338692 0.01351964\n",
      " 0.01365366 0.01378899 0.01392564 0.01406363 0.01420296 0.01434366\n",
      " 0.01448572 0.01462918 0.01477403 0.0149203  0.01506799 0.01521712\n",
      " 0.01536771 0.01551976 0.01567329 0.01582831 0.01598485 0.01614291\n",
      " 0.0163025  0.01646364 0.01662636 0.01679065 0.01695654 0.01712403\n",
      " 0.01729316 0.01746392 0.01763634 0.01781043 0.01798621 0.01816369\n",
      " 0.01834289 0.01852382 0.01870651 0.01889096 0.0190772  0.01926523\n",
      " 0.01945508 0.01964677 0.01984031 0.02003571 0.020233   0.02043219\n",
      " 0.0206333  0.02083634 0.02104135 0.02124832 0.02145729 0.02166827\n",
      " 0.02188127 0.02209632 0.02231344 0.02253264 0.02275394 0.02297737\n",
      " 0.02320294 0.02343067 0.02366058 0.02389269 0.02412702 0.02436359\n",
      " 0.02460243 0.02484354 0.02508696 0.0253327  0.02558079 0.02583124\n",
      " 0.02608408 0.02633932 0.02659699 0.02685712 0.02711972 0.02738481\n",
      " 0.02765242 0.02792257 0.02819529 0.02847059 0.0287485  0.02902904\n",
      " 0.02931223 0.0295981  0.02988668 0.03017798 0.03047203 0.03076886\n",
      " 0.03106848 0.03137093 0.03167623 0.0319844  0.03229546 0.03260946\n",
      " 0.03292639 0.03324631 0.03356922 0.03389516 0.03422416 0.03455623\n",
      " 0.03489141 0.03522972 0.03557119 0.03591585 0.03626372 0.03661483\n",
      " 0.03696921 0.03732689 0.03768789 0.03805225 0.03841999 0.03879113\n",
      " 0.03916572 0.03954378 0.03992533 0.04031042 0.04069905 0.04109128\n",
      " 0.04148712 0.04188661 0.04228977 0.04269664 0.04310725 0.04352163\n",
      " 0.04393982 0.04436183 0.0447877  0.04521747 0.04565117 0.04608883\n",
      " 0.04653047 0.04697615 0.04742587 0.04787969 0.04833763 0.04879972\n",
      " 0.04926601 0.04973651 0.05021127 0.05069032 0.0511737  0.05166144\n",
      " 0.05215356 0.05265012 0.05315114 0.05365665 0.0541667  0.05468132\n",
      " 0.05520054 0.0557244  0.05625293 0.05678618 0.05732418 0.05786696\n",
      " 0.05841456 0.05896701 0.05952437 0.06008665 0.0606539  0.06122616\n",
      " 0.06180347 0.06238585 0.06297336 0.06356602 0.06416388 0.06476697\n",
      " 0.06537533 0.06598901 0.06660804 0.06723245 0.06786229 0.0684976\n",
      " 0.06913842 0.06978478 0.07043673 0.0710943  0.07175754 0.07242649\n",
      " 0.07310117 0.07378165 0.07446795 0.07516011 0.07585818 0.0765622\n",
      " 0.0772722  0.07798824 0.07871034 0.07943855 0.08017291 0.08091347\n",
      " 0.08166026 0.08241332 0.0831727  0.08393843 0.08471057 0.08548914\n",
      " 0.08627419 0.08706577 0.08786391 0.08866866 0.08948006 0.09029814\n",
      " 0.09112296 0.09195455 0.09279295 0.09363821 0.09449037 0.09534946\n",
      " 0.09621554 0.09708864 0.0979688  0.09885607 0.09975049 0.10065209\n",
      " 0.10156093 0.10247703 0.10340045 0.10433122 0.10526939 0.10621499\n",
      " 0.10716807 0.10812867 0.10909682 0.11007257 0.11105597 0.11204704\n",
      " 0.11304583 0.11405238 0.11506673 0.11608892 0.11711899 0.11815698\n",
      " 0.11920292 0.12025686 0.12131884 0.12238889 0.12346705 0.12455336\n",
      " 0.12564786 0.12675058 0.12786157 0.12898085 0.13010847 0.13124447\n",
      " 0.13238887 0.13354172 0.13470305 0.1358729  0.13705129 0.13823827\n",
      " 0.13943387 0.14063813 0.14185106 0.14307272 0.14430313 0.14554233\n",
      " 0.14679034 0.1480472  0.14931293 0.15058758 0.15187116 0.15316372\n",
      " 0.15446527 0.15577584 0.15709547 0.15842418 0.159762   0.16110895\n",
      " 0.16246506 0.16383036 0.16520487 0.16658861 0.16798161 0.1693839\n",
      " 0.17079548 0.17221639 0.17364665 0.17508627 0.17653527 0.17799369\n",
      " 0.17946152 0.18093879 0.18242552 0.18392173 0.18542742 0.18694261\n",
      " 0.18846733 0.19000157 0.19154535 0.19309868 0.19466158 0.19623406\n",
      " 0.19781611 0.19940776 0.201009   0.20261985 0.2042403  0.20587037\n",
      " 0.20751006 0.20915937 0.21081829 0.21248684 0.21416502 0.21585281\n",
      " 0.21755022 0.21925725 0.22097389 0.22270014 0.22443599 0.22618143\n",
      " 0.22793645 0.22970105 0.23147522 0.23325894 0.2350522  0.23685498\n",
      " 0.23866729 0.24048908 0.24232036 0.2441611  0.24601128 0.24787089\n",
      " 0.24973989 0.25161828 0.25350602 0.25540308 0.25730945 0.2592251\n",
      " 0.26114999 0.2630841  0.2650274  0.26697985 0.26894142 0.27091208\n",
      " 0.27289178 0.2748805  0.27687819 0.27888482 0.28090034 0.28292471\n",
      " 0.28495789 0.28699984 0.2890505  0.29110983 0.29317778 0.2952543\n",
      " 0.29733935 0.29943286 0.30153478 0.30364507 0.30576366 0.3078905\n",
      " 0.31002552 0.31216867 0.31431989 0.31647911 0.31864627 0.3208213\n",
      " 0.32300414 0.32519473 0.32739298 0.32959884 0.33181223 0.33403307\n",
      " 0.3362613  0.33849684 0.34073961 0.34298954 0.34524654 0.34751054\n",
      " 0.34978145 0.3520592  0.35434369 0.35663485 0.35893259 0.36123682\n",
      " 0.36354746 0.36586441 0.36818758 0.37051689 0.37285223 0.37519353\n",
      " 0.37754067 0.37989357 0.38225213 0.38461624 0.38698582 0.38936077\n",
      " 0.39174097 0.39412633 0.39651675 0.39891212 0.40131234 0.4037173\n",
      " 0.4061269  0.40854102 0.41095957 0.41338242 0.41580948 0.41824062\n",
      " 0.42067575 0.42311474 0.42555748 0.42800387 0.43045378 0.4329071\n",
      " 0.43536371 0.4378235  0.44028635 0.44275215 0.44522076 0.44769209\n",
      " 0.450166   0.45264238 0.45512111 0.45760206 0.46008512 0.46257015\n",
      " 0.46505705 0.46754569 0.47003595 0.4725277  0.47502081 0.47751518\n",
      " 0.48001066 0.48250714 0.4850045  0.4875026  0.49000133 0.49250056\n",
      " 0.49500017 0.49750002 0.5        0.50249998 0.50499983 0.50749944\n",
      " 0.50999867 0.5124974  0.5149955  0.51749286 0.51998934 0.52248482\n",
      " 0.52497919 0.5274723  0.52996405 0.53245431 0.53494295 0.53742985\n",
      " 0.53991488 0.54239794 0.54487889 0.54735762 0.549834   0.55230791\n",
      " 0.55477924 0.55724785 0.55971365 0.5621765  0.56463629 0.5670929\n",
      " 0.56954622 0.57199613 0.57444252 0.57688526 0.57932425 0.58175938\n",
      " 0.58419052 0.58661758 0.58904043 0.59145898 0.5938731  0.5962827\n",
      " 0.59868766 0.60108788 0.60348325 0.60587367 0.60825903 0.61063923\n",
      " 0.61301418 0.61538376 0.61774787 0.62010643 0.62245933 0.62480647\n",
      " 0.62714777 0.62948311 0.63181242 0.63413559 0.63645254 0.63876318\n",
      " 0.64106741 0.64336515 0.64565631 0.6479408  0.65021855 0.65248946\n",
      " 0.65475346 0.65701046 0.65926039 0.66150316 0.6637387  0.66596693\n",
      " 0.66818777 0.67040116 0.67260702 0.67480527 0.67699586 0.6791787\n",
      " 0.68135373 0.68352089 0.68568011 0.68783133 0.68997448 0.6921095\n",
      " 0.69423634 0.69635493 0.69846522 0.70056714 0.70266065 0.7047457\n",
      " 0.70682222 0.70889017 0.7109495  0.71300016 0.71504211 0.71707529\n",
      " 0.71909966 0.72111518 0.72312181 0.7251195  0.72710822 0.72908792\n",
      " 0.73105858 0.73302015 0.7349726  0.7369159  0.73885001 0.7407749\n",
      " 0.74269055 0.74459692 0.74649398 0.74838172 0.75026011 0.75212911\n",
      " 0.75398872 0.7558389  0.75767964 0.75951092 0.76133271 0.76314502\n",
      " 0.7649478  0.76674106 0.76852478 0.77029895 0.77206355 0.77381857\n",
      " 0.77556401 0.77729986 0.77902611 0.78074275 0.78244978 0.78414719\n",
      " 0.78583498 0.78751316 0.78918171 0.79084063 0.79248994 0.79412963\n",
      " 0.7957597  0.79738015 0.798991   0.80059224 0.80218389 0.80376594\n",
      " 0.80533842 0.80690132 0.80845465 0.80999843 0.81153267 0.81305739\n",
      " 0.81457258 0.81607827 0.81757448 0.81906121 0.82053848 0.82200631\n",
      " 0.82346473 0.82491373 0.82635335 0.82778361 0.82920452 0.8306161\n",
      " 0.83201839 0.83341139 0.83479513 0.83616964 0.83753494 0.83889105\n",
      " 0.840238   0.84157582 0.84290453 0.84422416 0.84553473 0.84683628\n",
      " 0.84812884 0.84941242 0.85068707 0.8519528  0.85320966 0.85445767\n",
      " 0.85569687 0.85692728 0.85814894 0.85936187 0.86056613 0.86176173\n",
      " 0.86294871 0.8641271  0.86529695 0.86645828 0.86761113 0.86875553\n",
      " 0.86989153 0.87101915 0.87213843 0.87324942 0.87435214 0.87544664\n",
      " 0.87653295 0.87761111 0.87868116 0.87974314 0.88079708 0.88184302\n",
      " 0.88288101 0.88391108 0.88493327 0.88594762 0.88695417 0.88795296\n",
      " 0.88894403 0.88992743 0.89090318 0.89187133 0.89283193 0.89378501\n",
      " 0.89473061 0.89566878 0.89659955 0.89752297 0.89843907 0.89934791\n",
      " 0.90024951 0.90114393 0.9020312  0.90291136 0.90378446 0.90465054\n",
      " 0.90550963 0.90636179 0.90720705 0.90804545 0.90887704 0.90970186\n",
      " 0.91051994 0.91133134 0.91213609 0.91293423 0.91372581 0.91451086\n",
      " 0.91528943 0.91606157 0.9168273  0.91758668 0.91833974 0.91908653\n",
      " 0.91982709 0.92056145 0.92128966 0.92201176 0.9227278  0.9234378\n",
      " 0.92414182 0.92483989 0.92553205 0.92621835 0.92689883 0.92757351\n",
      " 0.92824246 0.9289057  0.92956327 0.93021522 0.93086158 0.9315024\n",
      " 0.93213771 0.93276755 0.93339196 0.93401099 0.93462467 0.93523303\n",
      " 0.93583612 0.93643398 0.93702664 0.93761415 0.93819653 0.93877384\n",
      " 0.9393461  0.93991335 0.94047563 0.94103299 0.94158544 0.94213304\n",
      " 0.94267582 0.94321382 0.94374707 0.9442756  0.94479946 0.94531868\n",
      " 0.9458333  0.94634335 0.94684886 0.94734988 0.94784644 0.94833856\n",
      " 0.9488263  0.94930968 0.94978873 0.95026349 0.95073399 0.95120028\n",
      " 0.95166237 0.95212031 0.95257413 0.95302385 0.95346953 0.95391117\n",
      " 0.95434883 0.95478253 0.9552123  0.95563817 0.95606018 0.95647837\n",
      " 0.95689275 0.95730336 0.95771023 0.95811339 0.95851288 0.95890872\n",
      " 0.95930095 0.95968958 0.96007467 0.96045622 0.96083428 0.96120887\n",
      " 0.96158001 0.96194775 0.96231211 0.96267311 0.96303079 0.96338517\n",
      " 0.96373628 0.96408415 0.96442881 0.96477028 0.96510859 0.96544377\n",
      " 0.96577584 0.96610484 0.96643078 0.96675369 0.96707361 0.96739054\n",
      " 0.96770454 0.9680156  0.96832377 0.96862907 0.96893152 0.96923114\n",
      " 0.96952797 0.96982202 0.97011332 0.9704019  0.97068777 0.97097096\n",
      " 0.9712515  0.97152941 0.97180471 0.97207743 0.97234758 0.97261519\n",
      " 0.97288028 0.97314288 0.97340301 0.97366068 0.97391592 0.97416876\n",
      " 0.97441921 0.9746673  0.97491304 0.97515646 0.97539757 0.97563641\n",
      " 0.97587298 0.97610731 0.97633942 0.97656933 0.97679706 0.97702263\n",
      " 0.97724606 0.97746736 0.97768656 0.97790368 0.97811873 0.97833173\n",
      " 0.97854271 0.97875168 0.97895865 0.97916366 0.9793667  0.97956781\n",
      " 0.979767   0.97996429 0.98015969 0.98035323 0.98054492 0.98073477\n",
      " 0.9809228  0.98110904 0.98129349 0.98147618 0.98165711 0.98183631\n",
      " 0.98201379 0.98218957 0.98236366 0.98253608 0.98270684 0.98287597\n",
      " 0.98304346 0.98320935 0.98337364 0.98353636 0.9836975  0.98385709\n",
      " 0.98401515 0.98417169 0.98432671 0.98448024 0.98463229 0.98478288\n",
      " 0.98493201 0.9850797  0.98522597 0.98537082 0.98551428 0.98565634\n",
      " 0.98579704 0.98593637 0.98607436 0.98621101 0.98634634 0.98648036\n",
      " 0.98661308 0.98674452 0.98687468 0.98700358 0.98713124 0.98725765\n",
      " 0.98738284 0.98750681 0.98762959 0.98775117 0.98787157 0.9879908\n",
      " 0.98810887 0.98822579 0.98834158 0.98845625 0.9885698  0.98868224\n",
      " 0.98879359 0.98890386 0.98901306 0.98912119 0.98922827 0.98933431\n",
      " 0.98943931 0.98954329 0.98964626 0.98974823 0.9898492  0.98994919\n",
      " 0.9900482  0.99014624 0.99024333 0.99033948 0.99043468 0.99052896\n",
      " 0.99062231 0.99071475 0.99080629 0.99089694 0.9909867  0.99107558\n",
      " 0.9911636  0.99125075 0.99133706 0.99142251 0.99150714 0.99159093\n",
      " 0.99167391 0.99175607 0.99183743 0.99191799 0.99199777 0.99207676\n",
      " 0.99215498 0.99223243 0.99230912 0.99238507 0.99246027 0.99253473\n",
      " 0.99260846 0.99268147 0.99275376 0.99282534 0.99289623 0.99296641\n",
      " 0.99303591 0.99310473 0.99317287 0.99324034]\n",
      "Predicted Output: \n",
      " [[0.37771334]\n",
      " [0.41477855]\n",
      " [0.40348484]]\n",
      "Loss: \n",
      " 0.16174922917818682\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,1000) and (1,3) not aligned: 1000 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-26a4b07972d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted Output: \\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-0916dd48dba9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-0916dd48dba9>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, X, y, o)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# z2 error: how much were our output layer weights off\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz2_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mo_delta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# z2 delta: how much were the weights off?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,1000) and (1,3) not aligned: 1000 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 500 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will implement backpropagation inside a multi-layer perceptron (aka a feedforward neural network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Stochastic Gradient Descent (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "A true Stochastic GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X,y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA9LaTgKr6rP"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_kHb6Se1u9y"
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYYVhFf4rn3q"
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "L-gYdVfgrysE",
    "outputId": "ae371bf9-692c-49b4-b165-8562dab9c06e"
   },
   "outputs": [],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Jyv_L8Z2sKOA",
    "outputId": "08725651-6d21-401b-85c0-3487370b8bc0"
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Gtf9WI9FtGPk",
    "outputId": "d062b2a3-5a92-403e-8ce0-c070aa79907b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "This is a reference implementation for you to explore. You will not be expected to apply it to today's module project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Sequential API (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:\n",
    "\n",
    "> Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIJoRBxHy27n"
   },
   "source": [
    "### Keras Perceptron Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "X = df[['x1', 'x2']].values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5216
    },
    "colab_type": "code",
    "id": "TQxyONqKvFxB",
    "outputId": "12966e66-2297-4f82-85b3-c275a9c38563"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# This is our perceptron from Monday's by-hand: \n",
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Z1wfKUxszPKa",
    "outputId": "0cdacd1d-6e5a-4bbe-fabb-568cd94724be"
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In the `Sequential` api model, you specify a model architecture by 'sequentially specifying layers. This type of specification works well for feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). The Keras `Sequential` API follows a standardarized worklow to estimate a 'net: \n",
    "\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Compile Model\n",
    "4. Fit Model\n",
    "5. Evaluate Model\n",
    "\n",
    "You saw these steps in our Keras Perceptron Sample, but let's walk thru each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md5D67XwqVAf",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "bn09phMBpY1J",
    "outputId": "1c45fb6a-e3cb-4ec1-fb85-de52b3c60bae"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Stretch - use dropout \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Variable Types\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') /255.\n",
    "\n",
    "# Correct Encoding on Y\n",
    "# What softmax expects = [0,0,0,0,0,1,0,0,0,0]\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0xMqOyTs5xt"
   },
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp9USczrfu6M"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAzHLg27thoN"
   },
   "source": [
    "I'll instantiate my model as a \"sequential\" model. This just means that I'm going to tell Keras what my model's architecture should be one layer at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSNsL49Xp6KI"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCYX6QzJtvpG"
   },
   "source": [
    "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
    "\n",
    " > [\"Just your regular densely-connected NN layer.\"](https://keras.io/layers/core/)\n",
    " \n",
    " The first argument is how many neurons we want to have in that layer. To create a perceptron-esque model we will just set it to 10. Our architecture is just an input and output layer. We will tell it that there will be 784 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "GNzOLidxtvFa",
    "outputId": "35b1457d-0189-49f1-aa6d-3ef15b29bd6e"
   },
   "outputs": [],
   "source": [
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(10,activation=\"softmax\")) #Relu is valid option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnI3jwKMtBL2",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Compile Model\n",
    "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like `categorical_crossentropy`. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training.\n",
    "\n",
    "#### Adam Optimizer\n",
    "Check out this links for more background on the Adam optimizer and Stohastic Gradient Descent\n",
    "* [Adam Optimization Algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "* [Adam Optimizer - original paper](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qp6xwYaqurRO"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dW8SZ2Ls9SX",
    "toc-hr-collapsed": false
   },
   "source": [
    "### Fit Model\n",
    "\n",
    "Lets train it up! `model.fit()` has a `batch_size` parameter that we can use if we want to do mini-batch epochs, but since this tabular dataset is pretty small we're just going to delete that parameter. Keras' default `batch_size` is `None` so omiting it will tell Keras to do batch epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test,y_test)\n",
    "print(\"\\n\")\n",
    "print(\"Validation Data Metrics:\")\n",
    "print(f\"{model.metrics_names[0]}: {scores[0]}\")\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHYB7k9q3O8T"
   },
   "source": [
    "### Unstable Results\n",
    "\n",
    "You'll notice that if we rerun the results might differ from the origin run. This can be explain by a bunch of factors. Check out some of them in this article: \n",
    "\n",
    "<https://machinelearningmastery.com/randomness-in-machine-learning/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to leverage the Keras `Sequential` api to estimate a feed forward neural networks on a dataset.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
